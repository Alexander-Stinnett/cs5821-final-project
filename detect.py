# CODE GENERATED BY CHATGPT!
# binoculars_detect.py
# ----------------------------------------------------
# Classifies text files as Human vs LLM using Binoculars
# Using Falcon-7B-Instruct (observer, GPU) and Falcon-7B (performer, CPU)
# ----------------------------------------------------

import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM

# ------------------------------
# Device setup
# ------------------------------
DEVICE_OBS = "cuda:0" if torch.cuda.is_available() else "cpu"
DEVICE_PERF = "cpu"   # performer forced to CPU

observer_name = "tiiuae/falcon-7b-instruct"
performer_name = "tiiuae/falcon-7b"

# ------------------------------
# Load tokenizer and models
# ------------------------------
print("Loading models... This may take some time.")

tokenizer = AutoTokenizer.from_pretrained(observer_name)
tokenizer.pad_token = tokenizer.eos_token

# Observer: GPU
observer = AutoModelForCausalLM.from_pretrained(observer_name).to(DEVICE_OBS)

# Performer: CPU
performer = AutoModelForCausalLM.from_pretrained(performer_name).to(DEVICE_PERF)

observer.eval()
performer.eval()

loss_fn = torch.nn.CrossEntropyLoss(reduction='none')
softmax_fn = torch.nn.Softmax(dim=-1)


# ------------------------------
# Core functions
# ------------------------------
def encode(batch_text):
    return tokenizer(
        batch_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512,
    )


@torch.inference_mode()
def get_logits(encoding):
    input_ids = encoding.input_ids
    attention = encoding.attention_mask

    # Observer (GPU)
    obs_logits = observer(
        input_ids=input_ids.to(DEVICE_OBS),
        attention_mask=attention.to(DEVICE_OBS)
    ).logits

    # Performer (CPU)
    perf_logits = performer(
        input_ids=input_ids.to(DEVICE_PERF),
        attention_mask=attention.to(DEVICE_PERF)
    ).logits

    return obs_logits, perf_logits


def perplexity(encoding, logits):
    shifted_logits = logits[..., :-1, :]
    shifted_labels = encoding.input_ids[..., 1:]
    shifted_mask = encoding.attention_mask[..., 1:]

    ppl = loss_fn(
        shifted_logits.transpose(1, 2).cpu(),
        shifted_labels.cpu()
    )

    ppl = (ppl * shifted_mask.cpu()).sum(1) / shifted_mask.sum(1)
    return ppl.numpy()


def cross_perplexity(observer_logits, performer_logits, encoding):
    B, S, V = observer_logits.shape

    performer_probs = softmax_fn(performer_logits).view(B * S, V).cpu()
    observer_scores = observer_logits.view(B * S, V).cpu()

    xppl = loss_fn(observer_scores[:-1], performer_probs[:-1])
    xppl = xppl.view(B, S - 1)

    padding_mask = (encoding.input_ids != tokenizer.pad_token_id).cpu()
    xppl = (xppl * padding_mask[:, :-1]).sum(1) / padding_mask[:, :-1].sum(1)

    return xppl.numpy()


def binocular_score(text):
    encoding = encode([text])
    obs_logits, perf_logits = get_logits(encoding)

    ppl = perplexity(encoding, obs_logits)
    xppl = cross_perplexity(obs_logits, perf_logits, encoding)

    return float(ppl[0] / xppl[0])


# ------------------------------
# Evaluate folder of text files
# ------------------------------
def classify_folder(folder_path, threshold=0.90):
    results = []
    files = sorted(f for f in os.listdir(folder_path) if f.endswith(".txt"))

    for fname in files:
        full = os.path.join(folder_path, fname)
        with open(full, "r", encoding="utf-8") as f:
            text = f.read()

        score = binocular_score(text)
        label = "Human" if score >= threshold else "LLM"

        results.append((fname, score, label))

    return results


if __name__ == "__main__":
    folder = "texts/"  # put your text files here
    output = classify_folder(folder)

    print("\nFILENAME | SCORE | CLASS")
    for f, s, label in output:
        print(f"{f:10s} | {s:.4f} | {label}")
